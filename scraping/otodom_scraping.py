# scraping/otodom_locations.py
import asyncio
import csv
import random
import re
import time
from pathlib import Path
from typing import List, Optional, Tuple, Iterable, Dict, Set

from playwright.async_api import async_playwright, Page

LISTING_BASE = ("https://www.otodom.pl/pl/wyniki/wynajem/mieszkanie/"
                "malopolskie/krakow/krakow/krakow?limit=72&by=DEFAULT&direction=DESC")

# --- Konfiguracja scrapowania ---
TARGET_OFFERS = 400  # Ile ofert chcemy zebraÄ‡ (moÅ¼e byÄ‡ wiÄ™cej niÅ¼ na jednej stronie)
OFFERS_PER_PAGE =72  # Ile ofert jest na jednej stronie
SAVE_EVERY = 10  # zapisuj co X rekordÃ³w
TITLES_FILE = "../data/seen_titles.txt"
BLACKLIST_FILE = "../data/blacklist.txt"
TITLES_LOCK = asyncio.Lock()

# --- sÅ‚owniki pomocnicze ---
DISTRICTS = {
    # 18 dzielnic + dzielnice-zespoÅ‚y
    "Stare Miasto", "GrzegÃ³rzki", "PrÄ…dnik Czerwony", "PrÄ…dnik BiaÅ‚y", "Krowodrza",
    "Bronowice", "Zwierzyniec", "DÄ™bniki", "Åagiewniki-Borek FaÅ‚Ä™cki",
    "Swoszowice", "PodgÃ³rze Duchackie", "BieÅ¼anÃ³w-Prokocim", "PodgÃ³rze",
    "CzyÅ¼yny", "Mistrzejowice", "BieÅ„czyce", "WzgÃ³rza KrzesÅ‚awickie", "Nowa Huta",
    # czÄ™sto uÅ¼ywane zespoÅ‚y/osiedla
    "Kazimierz", "ZabÅ‚ocie", "PÅ‚aszÃ³w", "Ruczaj", "Skotniki", "Kobierzyn",
    "ÅobzÃ³w", "Kleparz", "Piasek", "Nowy Åšwiat", "Salwator", "Olsza",
    "Azory", "Å»abiniec", "Tonie", "Wola Duchacka", "ÅÄ™g", "Bielany",
    "Stradom", "LudwinÃ³w", "DÄ…bie", "Borek FaÅ‚Ä™cki", "Prokocim", "KozÅ‚Ã³wek",
    "PrzewÃ³z", "BieÅ¼anÃ³w", "Krowodrza GÃ³rka"
}
CITY_TOKENS = {"krakÃ³w", "maÅ‚opolskie"}

# --- Detekcja blokady CloudFront ---
class CloudfrontBlocked(Exception):
    pass

class CloudflareBlocked(Exception):
    pass

async def _detect_cloudfront_block(page: Page):
    try:
        html = await page.content()
        if ("Request blocked" in html and "CloudFront" in html) or "The request could not be satisfied" in html:
            raise CloudfrontBlocked("CloudFront 403 block detected")
    except CloudfrontBlocked:
        raise
    except Exception:
        # wÄ…tpliwe, ale nie przerywaj
        return

async def _detect_cloudflare_block(page: Page):
    """ZgÅ‚asza CloudflareBlocked tylko przy twardych markerach blokady."""
    try:
        html = await page.content()
        title = (await page.title() or "").lower()

        # twarde markery Cloudflare/challenge
        hard_markers = (
            'cf-browser-verification',    # class/id
            'data-cf-beacon',             # beacon
            'cf-challenge',               # script
            'cf-error-details',           # error page layout
            'jschl_vc', 'rchl_vc',        # (starsze challenge)
            'just a moment...',           # <title>
        )

        for m in hard_markers:
            if m in html.lower() or m in title:
                raise CloudflareBlocked(f"Hard CF marker: {m}")

        # CloudFront bÅ‚Ä™dy (403, Request blocked)
        cloudfront_markers = (
            '403 error',
            'the request could not be satisfied',
            'request blocked',
            'generated by cloudfront',
            'cloudfront documentation'
        )
        
        for m in cloudfront_markers:
            if m in html.lower():
                raise CloudfrontBlocked(f"CloudFront block: {m}")

        # â€miÄ™kkie" teksty â€“ NIE rzucamy wyjÄ…tku (mogÄ… faÅ‚szywie zapalaÄ‡ siÄ™)
        # 'Checking your browser', 'Please wait', 'Access denied' itd.
        # => tylko log
        soft_markers = ("checking your browser", "please wait while we verify", "access denied")
        for m in soft_markers:
            if m in html.lower() or m in title:
                print(f"[CF-soft] Marker: {m} (nie przerywam)")
                
    except (CloudflareBlocked, CloudfrontBlocked):
        raise
    except Exception as e:
        # Inne bÅ‚Ä™dy - loguj ale nie przerywaj
        print(f"[WARN] BÅ‚Ä…d podczas sprawdzania blokady: {e}")
        return

# --- Minimalny, ale szczelny ekstraktor adresu (geo-friendly) ---

from typing import Tuple

# Prefiksy ulicopodobne
_PREFIX = (
    r"(?:\bul\.?|\bulica\b|"          # ulica
    r"\bal\.?|\baleja\b|\balei\b|"    # aleja
    r"\bpl\.|\bplac\b|\bplacu\b|"     # plac
    r"\bos\.?|\bosiedle\b|\bosiedlu\b|" # osiedle
    r"\brondo\b|\brondzie\b|"         # rondo
    r"\brynek\b|\brynek\b|"           # rynek (z zapasowÄ… literÃ³wkÄ…)
    r"\bbulwary\b|\bbulwar\b)"        # bulwar(y)
)

# Uwaga: nazwÄ™ wymuszamy od DUÅ»EJ litery lub LICZBY (np. '29 Listopada')
_UC = "A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ»Å¹"
_NAME_WORD = rf"(?:(?-i:[{_UC}][\w\-Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)|\d+)"   # dodano wariant z liczbÄ…
_NUM        = r"(?:\d+[A-Za-z]?)"                       # numer domu
_NAME_SEQ_STRICT = rf"{_NAME_WORD}(?:\s+{_NAME_WORD}){{0,4}}(?:\s+{_NUM})?"  # np. '29 Listopada 98'

# 1) Prefiksowane: 'ul./al./pl./os./rondo/rynek/bulwary ...'
PREF_RE = re.compile(rf"{_PREFIX}\s+(?P<name>{_NAME_SEQ_STRICT})", re.U | re.I)

# 2) â€GoÅ‚e" nazwy z numerem â€” TYLKO w bardzo bezpiecznych kontekstach.
#    a) linia zaczyna siÄ™ od: Adres/ Ulica / ul.
UNP_LINE_RE = re.compile(
    rf"^\s*(?:adres|ulica|ul\.?)\s*[:\-]?\s*(?P<name>{_NAME_WORD}(?:\s+{_NAME_WORD}){{0,4}})\s+(?P<num>{_NUM})\b",
    re.U | re.M | re.I
)
#    b) fraza 'przy (ul.|ulicy|alei|placu|os.) XYZ 12'
UNP_PRZY_RE = re.compile(
    rf"\bprzy\s+(?:(?:ul\.?|ulicy|alei|placu|os\.?)\s+)?(?P<name>{_NAME_WORD}(?:\s+{_NAME_WORD}){{0,4}})\s+(?P<num>{_NUM})\b",
    re.U | re.I
)

# Å›mieci na ogonie nazwy (odetnij wszystko od tych znacznikÃ³w)
_TAIL_NOISE_RE = re.compile(
    r"\b(eng|english|below|pl|ua|ru|studio|pets?|friendly|co-?work(?:ing)?|bez\s*prowizji|bezpoÅ›rednio)\b.*$",
    re.I | re.U
)

# --- metraÅ¼ (m2) ---

_AREA_UNIT_RE = (
    r"(?:m2|m\^2|m\s*2|mÂ²|mkw|m\.\s*kw|m\s*kw|"
    r"metr(?:Ã³w|y)?\s*kw(?:\.|adratow(?:e|ych)?)?)"
)
_AREA_LABEL_RE = r"(?:powierzch\w*|metra(?:Å¼|z)|pow\.)"

def _norm_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").replace("\u00A0"," ")).strip()

# --- Helpery do obsÅ‚ugi pliku tytuÅ‚Ã³w ---
def _norm_title(s: Optional[str]) -> str:
    if not s:
        return ""
    t = s.lower().replace("\u00a0"," ")
    t = re.sub(r"\s+", " ", t).strip()
    # wytnij metraÅ¼e i boilerplate
    t = re.sub(r"\b\d{1,3}(?:[.,]\d{1,2})?\s*(?:m2|mÂ²|m\s*2|m\s*kw|m\.\s*kw|mkw)\b", "", t)
    for w in ("do wynajÄ™cia","wynajem","bez prowizji","bezpoÅ›rednio","english below","eng","oferta","ogÅ‚oszenie","krakÃ³w","krakowie"):
        t = t.replace(w, " ")
    t = re.sub(r"[^\wÄ…Ä‡Ä™Å‚Å„Ã³Å›Å¼Åº\s-]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def load_seen_titles(path: str) -> Set[str]:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    if not p.exists():
        p.touch()
        return set()
    with p.open("r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}

def load_blacklist(path: str) -> Set[str]:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    if not p.exists():
        p.touch()
        return set()
    with p.open("r", encoding="utf-8") as f:
        return {line.strip() for line in f if line.strip()}

async def append_seen_title(path: str, title_norm: str, lock: asyncio.Lock, seen: Set[str]):
    if not title_norm:
        return
    async with lock:
        if title_norm in seen:
            return
        with open(path, "a", encoding="utf-8") as f:
            f.write(title_norm + "\n")
        seen.add(title_norm)

async def append_to_blacklist(path: str, title_norm: str, url: str, lock: asyncio.Lock, blacklist: Set[str]):
    if not title_norm:
        return
    async with lock:
        if title_norm in blacklist:
            return
        with open(path, "a", encoding="utf-8") as f:
            f.write(f"{title_norm}\t{url}\n")
        blacklist.add(title_norm)

def _to_float_area(num_str: str) -> Optional[float]:
    if not num_str: return None
    try:
        v = float(num_str.replace(",", ".").replace(" ", ""))
        # sanity: mieszkania w wideÅ‚kach ~10â€“300 m2
        return v if 10 <= v <= 300 else None
    except:
        return None

def _area_from_text(text: str) -> Optional[float]:
    """OgÃ³lny parser 'NN[,.]N unit' z sensownymi jednostkami."""
    t = _norm_spaces(text).lower()
    m = re.search(rf"(\d{{1,3}}(?:[.,]\d{{1,2}})?)\s*{_AREA_UNIT_RE}\b", t, flags=re.I)
    return _to_float_area(m.group(1)) if m else None

def _area_from_labeled(text: str) -> Optional[float]:
    """Parser z etykietÄ… 'Powierzchnia:' â€“ jednostka moÅ¼e byÄ‡ pominiÄ™ta."""
    t = _norm_spaces(text).lower()
    # np. 'Powierzchnia: ok. 41,60 mÂ²' lub 'Pow.: 42'
    m = re.search(
        rf"{_AREA_LABEL_RE}\s*[:=\-]?\s*(?:ok\.?\s*)?"
        rf"(\d{{1,3}}(?:[.,]\d{{1,2}})?)\s*(?:{_AREA_UNIT_RE})?",
        t, flags=re.I
    )
    return _to_float_area(m.group(1)) if m else None

async def extract_area_m2(page: Page, title: Optional[str], desc: str) -> Tuple[Optional[float], str]:
    """Zwraca (metraÅ¼_m2, ÅºrÃ³dÅ‚o: 'tabela'|'tytuÅ‚'|'opis'|'')"""
    # 1) TABELA â€SzczegÃ³Å‚y" â†’ znajdÅº wiersz, gdzie lewa kolumna zawiera 'Powierzch'
    try:
        rows = await page.query_selector_all('[data-sentry-element="ItemGridContainer"]')
        for r in rows:
            cells = await r.query_selector_all('div')
            if len(cells) >= 2:
                lab = _norm_spaces(await cells[0].inner_text()).lower()
                if "powierzch" in lab:  # â€Powierzchnia:"
                    val = _norm_spaces(await cells[1].inner_text())
                    area = _area_from_text(val) or _area_from_labeled(val)
                    if area: 
                        return area, "tabela"
    except:
        pass

    # 2) TytuÅ‚ (czÄ™sto â€41,60 mÂ²")
    if title:
        area = _area_from_labeled(title) or _area_from_text(title)
        if area: 
            return area, "tytuÅ‚"

    # 3) Opis â€“ najpierw wersja z etykietÄ…, potem ogÃ³lna 'NN mÂ²'
    if desc:
        area = _area_from_labeled(desc) or _area_from_text(desc)
        if area: 
            return area, "opis"

    return None, ""

def _preclean_for_match(text: str) -> str:
    """RozwiÅ„ skrÃ³ty (pÅ‚k., pil., gen., mjr., kpt., dr., prof., ks., Å›w.) zanim dopasujemy regex."""
    s = text or ""
    repl = [
        (r"\bppÅ‚k\.\s*", "PodpuÅ‚kownika "),
        (r"\bpÅ‚k\.\s*",  "PuÅ‚kownika "),
        (r"\bpil\.\s*",  "Pilota "),
        (r"\bgen\.\s*",  "GeneraÅ‚a "),
        (r"\bmjr\.\s*",  "Majora "),
        (r"\bmaj\.\s*",  "Majora "),
        (r"\bkpt\.\s*",  "Kapitana "),
        (r"\bdr\.\s*",   "Doktora "),
        (r"\bprof\.\s*", "Profesora "),
        (r"\bks\.\s*",   "KsiÄ™dza "),
        (r"\bÅ›w\.\s*",   "Åšw. "),
    ]
    for pat, rep in repl:
        s = re.sub(pat, rep, s, flags=re.I)
    return s

def _fix_titles(s: str) -> str:
    s = re.sub(r"\bSw\.?\b", "Åšw.", s, flags=re.I)
    s = re.sub(r"\bgen\.?\b", "GeneraÅ‚a", s, flags=re.I)
    s = re.sub(r"\bpÅ‚k\.?\b", "PuÅ‚kownika", s, flags=re.I)
    s = re.sub(r"\bprof\.?\b", "Profesora", s, flags=re.I)
    s = re.sub(r"\bdr\.?\b", "Doktora", s, flags=re.I)
    s = " ".join(w[:1].upper()+w[1:] for w in s.split())
    # rzymskie
    s = re.sub(r"\bIi\b","II", s); s = re.sub(r"\bIii\b","III", s); s = re.sub(r"\bIv\b","IV", s)
    # znormalizuj przypadki z 'GeneraÅ‚a.' â†’ 'GeneraÅ‚a '
    s = re.sub(r"\b(GeneraÅ‚a|PuÅ‚kownika|Doktora|Profesora|KsiÄ™dza)\.\s+", r"\1 ", s)
    return s

def _canon_prefix(p: str) -> str:
    p = p.lower()
    if p.startswith("ul"): return "ul."
    if p.startswith("al"): return "al."
    if p.startswith("pl"): return "pl."
    if p.startswith("os"): return "os."
    if p.startswith("rondo"): return "rondo"
    if p.startswith("rynek"): return "rynek"
    if p.startswith("bulwar"): return "bulwary"
    return p

def _drop_redundant_noun(prefix: str, name: str) -> Tuple[str, str]:
    """Usuwa 'Ulica/Aleja/Plac/Osiedle/Rondo/Rynek' z poczÄ…tku nazwy jeÅ›li powtÃ³rzone
       i ewentualnie koryguje prefiks, gdy z UNPREFIXED zrobiliÅ›my omyÅ‚kowo 'ul.'."""
    if not name: return prefix, name
    first = name.split()[0].lower()
    # mapa: 'sÅ‚owo w nazwie' â†’ (docelowy prefiks, czy usuwaÄ‡ pierwszy token z nazwy)
    leading = {
        "ulica": ("ul.", True), "ulicy": ("ul.", True),
        "aleja": ("al.", True), "alei": ("al.", True),
        "plac": ("pl.", True), "placu": ("pl.", True),
        "osiedle": ("os.", True), "osiedlu": ("os.", True), "os.": ("os.", True),
        "rondo": ("rondo", True), "rondzie": ("rondo", True),
        "rynek": ("rynek", True), "rynku": ("rynek", True),
        "bulwar": ("bulwary", True), "bulwary": ("bulwary", True),
    }
    if first in leading:
        new_pref, drop = leading[first]
        if prefix != new_pref: prefix = new_pref
        if drop: name = " ".join(name.split()[1:])
    # Specjalny przypadek: 'al. Aleja â€¦' / 'ul. Ulica â€¦'
    if prefix == "al." and first in {"aleja","alei"}:
        name = " ".join(name.split()[1:])
    if prefix == "ul." and first in {"ulica","ulicy"}:
        name = " ".join(name.split()[1:])
    return prefix, name

def _strip_tail_noise(name: str) -> str:
    return _TAIL_NOISE_RE.sub("", name).strip(" ,.;:-â€“â€”")

def _extract_prefixed_first(text: str, *, forbid_rynek: bool = False) -> Optional[str]:
    if not text: 
        return None
    t = re.sub(r"\s+", " ", text)
    t = _preclean_for_match(t)  # â¬…ï¸ NOWE
    m = PREF_RE.search(t)
    if not m: 
        return None
    raw = m.group(0)
    prefix = _canon_prefix(raw.split()[0])
    if forbid_rynek and prefix in ("rynek", "rondo"):
        return None
    name = _fix_titles(m.group("name"))
    name = _strip_tail_noise(name)
    prefix, name = _drop_redundant_noun(prefix, name)
    return f"{prefix} {name}".strip() if name else None

def _extract_unprefixed_strict(line_text: str) -> Optional[str]:
    """Pracuje NA LINII (poczÄ…tek 'Adres/Ulica/ul.') lub fraza 'przy â€¦'."""
    if not line_text: return None
    t = re.sub(r"\s+", " ", line_text)
    for rx in (UNP_LINE_RE, UNP_PRZY_RE):
        m = rx.search(t)
        if m:
            name = _fix_titles(m.group("name"))
            num  = m.group("num")
            prefix = "ul."
            # skoryguj prefiks po wiodÄ…cym rzeczowniku (Aleja/Alei/Plac/Os./Rondo/Rynekâ€¦)
            prefix, name = _drop_redundant_noun(prefix, name)
            name = _strip_tail_noise(name)
            adr = f"{prefix} {name} {num}".strip()
            return adr
    return None

def _lines(desc: str) -> List[str]:
    raw = (desc or "").replace("â€¢", "\n")
    parts = [re.sub(r"\s+", " ", p).strip(" -") for p in raw.splitlines()]
    return [p for p in parts if p]

def _remove_prefix_for_csv(address: str) -> str:
    """Dla CSV zdejmujemy TYLKO 'ul.'; zostawiamy al./pl./os./rondo/rynek/bulwary."""
    if not address:
        return address

    # zdejmij wyÅ‚Ä…cznie 'ul.' z poczÄ…tku
    address = re.sub(r'^\s*ul\.\s+', '', address, flags=re.I)

    # usuÅ„ ', KrakÃ³w' na koÅ„cu
    address = re.sub(r',\s*KrakÃ³w\s*$', '', address, flags=re.I)

    # edge-case: 'al.  29' â†’ 'al. 29 Listopada' (jeÅ›li nie ma juÅ¼ 'Listopada')
    address = re.sub(r'^(al\.)\s*29\b(?!\s*Listopada)', r'\1 29 Listopada', address, flags=re.I)

    # porzÄ…dkuj wielokrotne spacje
    address = re.sub(r'\s+', ' ', address).strip()
    return address

def extract_address_for_geocode(header_loc: Optional[str], title: Optional[str], desc: str) -> Optional[str]:
    # 1) header/mapa â€“ NAJPIERW
    s = _extract_prefixed_first(header_loc or "")
    if s: return s + ", KrakÃ³w"
    # 2) tytuÅ‚
    s = _extract_prefixed_first(title or "")
    if s: return s + ", KrakÃ³w"
    s = _extract_unprefixed_strict(title or "")
    if s: return s + ", KrakÃ³w"
    # 3) opis (ale bez 'rynek' i 'rondo')
    for ln in _lines(desc):
        s = _extract_unprefixed_strict(ln) or _extract_prefixed_first(ln, forbid_rynek=True)
        if s: return s + ", KrakÃ³w"
    return None


def extract_district_from_breadcrumbs(breadcrumbs: List[str]) -> Optional[str]:
    # sprÃ³buj wprost z breadcrumbs
    for b in breadcrumbs:
        bt = b.strip()
        if bt and bt.lower() not in CITY_TOKENS and not bt.lower().startswith(("ul", "al", "pl", "os", "rondo", "rynek")):
            # wybierz pierwszy token, ktÃ³ry wyglÄ…da na dzielnicÄ™/osiedle
            if bt in DISTRICTS:
                return bt
    # fallback: weÅº ostatni â€œnie-miasto/nie-wojâ€ okruszek
    for b in reversed(breadcrumbs):
        bt = b.strip()
        if bt and bt.lower() not in CITY_TOKENS:
            return bt
    return None


def extract_id(url: str) -> str:
    m = re.search(r"-ID(\w+)$", url)
    return f"ID{m.group(1)}" if m else url.rsplit("/", 1)[-1]


def extract_district_from_text(text: str) -> Optional[str]:
    if not text:
        return None
    t = re.sub(r"\s+", " ", text)

    # 1) 'w dzielnicy X' / 'w dzielnicy Y-Z'
    m = re.search(r"w\s+dzielnic[yi]\s+([A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ»Å¹][\w\-Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+(?:\s+[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ»Å¹][\w\-Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)?)", t, flags=re.I)
    if m:
        cand = m.group(1).strip()
        if cand in DISTRICTS:
            return cand

    # 2) 'na/w <rejon>' (Kazimierzu, Ruczaju, Bronowicachâ€¦) â€“ sprawdzamy przeciwko sÅ‚ownikowi
    m2 = re.search(r"(?:na|w)\s+([A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ»Å¹][\w\-Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+(?:\s+[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ»Å¹][\w\-Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]+)?)", t, flags=re.I)
    if m2:
        cand = m2.group(1).strip().rstrip(".,;:â€“â€”")
        # sprostaÄ‡ odmianom: 'Kazimierzu'â†’'Kazimierz', 'Bronowicach'â†’'Bronowice'
        norm = re.sub(r"(u|ach|y|ie)$", "", cand)  # bardzo prosta normalizacja
        for item in DISTRICTS:
            if norm.lower() in {item.lower(), item.lower().rstrip("e"), item.lower().rstrip("y")}:
                return item

    return None


async def accept_cookies(page: Page):
    # rÃ³Å¼ne warianty przycisku
    selectors = [
        'button[data-testid="accept-cookies-button"]',
        'button:has-text("Akceptuj")',
        'button:has-text("Zgadzam")'
    ]
    for sel in selectors:
        try:
            btn = await page.query_selector(sel)
            if btn:
                await btn.click()
                await asyncio.sleep(0.8)
                break
        except:
            pass


async def collect_listing_entries_fast(page: Page) -> List[dict]:
    """
    Zwraca listÄ™: [{"url": "...", "title": "..."}] z listingu.
    UÅ¼ywa selektorÃ³w:
      a[data-cy="listing-item-link"]  zawiera wewnÄ…trz:
      p[data-cy="listing-item-title"] z tekstem tytuÅ‚u.
    """
    # Upewnij siÄ™, Å¼e listing siÄ™ narysowaÅ‚
    await page.wait_for_selector('a[data-cy="listing-item-link"]', timeout=20000)

    # Szybkie wyciÄ…ganie w kontekÅ›cie przeglÄ…darki â€“ 1 przebieg po DOM
    entries = await page.eval_on_selector_all(
        'a[data-cy="listing-item-link"]',
        """els => els.map(a => {
            const href = a.href; // absolutny URL
            const titleEl = a.querySelector('[data-cy="listing-item-title"]');
            const titleAttr = a.getAttribute('title') || a.getAttribute('aria-label') || '';
            const title = (titleEl?.innerText || titleAttr || '').trim();
            return { url: href, title };
        })"""
    )

    # UsuÅ„ duplikaty po URL, zachowujÄ…c kolejnoÅ›Ä‡
    const_seen = set()
    uniq = []
    for it in entries:
        if it["url"] in const_seen:
            continue
        uniq.append(it)
        const_seen.add(it["url"])
    return uniq


async def get_header_location(page: Page) -> Tuple[Optional[str], List[str]]:
    """Zwraca (tekst lokalizacji z headera/mapy lub breadcrumbs, breadcrumbs_list)."""
    loc = None

    # kilka wariantÃ³w linka/adresu w nagÅ‚Ã³wku
    candidates = [
        'a[data-cy="adPageLinkToMap"]',
        'a[href*="map"]',
        '[data-testid="adPageLocation"]',
        '[data-cy="adPageBreadcrumbs"] a[href*="map"]'
    ]
    for sel in candidates:
        el = await page.query_selector(sel)
        if el:
            try:
                t = (await el.inner_text()) or ""
                t = t.strip()
                if t:
                    loc = t
                    break
            except:
                pass

    breadcrumbs = []
    try:
        items = await page.query_selector_all('[data-cy="adPageBreadcrumbs"] li')
        for it in items:
            txt = (await it.inner_text()) or ""
            txt = txt.strip()
            if txt:
                breadcrumbs.append(txt)
        if not loc and breadcrumbs:
            loc = ", ".join(breadcrumbs)
    except:
        pass

    return loc, breadcrumbs


async def get_description_text(page: Page) -> str:
    # rÃ³Å¼ne warianty sekcji opisu
    selectors = [
        '[data-cy="adPageSectionDescription"]',
        '[data-cy="adPageAdDescription"]',
        'section:has(h2:has-text("Opis")), section:has(h2:has-text("OPIS"))'
    ]
    for sel in selectors:
        try:
            el = await page.query_selector(sel)
            if el:
                t = await el.inner_text()
                if t:
                    return t
        except:
            pass
    return ""


# --- ceny: najem + czynsz adm. ---

_AMOUNT_RE = r"(\d[\d\s\u00A0.,]*)"  # cyfry z odstÄ™pami/nbsp/kropkami/przecinkami
_CURRENCY_RE = r"(?:\s*(?:zÅ‚|pln))?"

def _to_int_pln(s: str) -> Optional[int]:
    """Z '2 300', '915 PLN', '790,00 zÅ‚' â†’ 2300/915/790 (int).
       JeÅ›li brak cyfr â€“ None."""
    if not s:
        return None
    digits = re.sub(r"[^\d]", "", s)
    return int(digits) if digits else None

def _first_amount(text: str) -> Optional[int]:
    """ZnajdÅº pierwszÄ… kwotÄ™ z walutÄ… (zÅ‚/PLN) â€“ zwraca int."""
    if not text:
        return None
    m = re.search(rf"{_AMOUNT_RE}\s*{_CURRENCY_RE}", text, flags=re.I)
    return _to_int_pln(m.group(1)) if m else None

def _parse_amount_after(label_re: str, text: str) -> Optional[int]:
    """ZnajdÅº kwotÄ™ po danym sÅ‚owie-kluczu (np. 'Czynsz administracyjny')."""
    if not text:
        return None
    m = re.search(rf"{label_re}\s*[:=\-]?\s*{_AMOUNT_RE}\s*{_CURRENCY_RE}", text, flags=re.I | re.U)
    return _to_int_pln(m.group(1)) if m else None

def _extract_rent_from_header_text(text: str) -> Optional[int]:
    """WyciÄ…ga gÅ‚Ã³wnÄ… cenÄ™ najmu z headera (fallback na wypadek zmian selektora)."""
    return _parse_amount_after(r"(?:cena|price)?", text)  # praktycznie: weÅº pierwszÄ… kwotÄ™

def _extract_admin_from_text(text: str, rent_hint: Optional[int]=None) -> Optional[int]:
    """Czynsz adm. z tekstu: preferuj wyraÅ¼enia 'czynsz administracyjny/opÅ‚aty adm.'.
       'czynsz' samotny bierzemy tylko jeÅ›li nie wyglÄ…da na 'czynsz najmu' i jest < rent_hint."""
    if not text:
        return None
    # 1) najmocniejsze sygnaÅ‚y
    for lab in (r"czynsz\s*administracyjny", r"opÅ‚aty?\s*administracyjne", r"adm\."):
        val = _parse_amount_after(lab, text)
        if val: return val

    # 2) â€+ Czynsz 790 zÅ‚", â€Czynsz: 850 zÅ‚" â€“ z wykluczeniem frazy â€najmu"
    out = None
    for m in re.finditer(rf"\bczynsz\b(?!\s*najmu)\s*[:=\-]?\s*{_AMOUNT_RE}\s*{_CURRENCY_RE}", text, flags=re.I):
        val = _to_int_pln(m.group(1))
        if not val:
            continue
        # Odfiltruj ew. â€czynsz najmu" (gdyby negatyw nie zadziaÅ‚aÅ‚ przez dziwne biaÅ‚e znaki)
        left = text[max(0, m.start()-30):m.start()].lower()
        right = text[m.end():m.end()+20].lower()
        if "najmu" in left or "najmu" in right:
            continue
        # Heurystyka: jeÅ›li mamy podpowiedÅº ceny najmu, a â€czynsz" >= najem*0.8 â€” pewnie to nie adm.
        if rent_hint and val >= int(0.8 * rent_hint):
            continue
        out = val
        break
    return out

async def extract_prices(page: Page, desc_text: str) -> Tuple[Optional[int], Optional[int], str]:
    """Zwraca (najem_pln, czynsz_adm_pln, ÅºrÃ³dÅ‚o_czynszu)"""
    rent = None
    admin = None
    admin_src = ""

    # --- 1) najem z headera ---
    try:
        price_el = await page.query_selector('[data-cy="adPageHeaderPrice"]')
        if price_el:
            rent_text = (await price_el.inner_text()) or ""
            rent = _first_amount(rent_text)
        if not rent:
            # fallback: caÅ‚y blok cenowy i sprÃ³buj wyÅ‚uskaÄ‡ pierwszÄ… kwotÄ™
            wrap = await page.query_selector('[data-sentry-element="PriceSection"]')
            if wrap:
                rent = _extract_rent_from_header_text(await wrap.inner_text())
    except:
        pass

    # --- 2) czynsz adm. z headera (np. â€+ Czynsz 790 zÅ‚") ---
    try:
        addl = await page.query_selector('[data-sentry-element="AdditionalPriceWrapper"]')
        if addl:
            t = (await addl.inner_text()) or ""
            admin = _extract_admin_from_text(t, rent)
            if admin:
                admin_src = "header"
        if not admin:
            # fallback: caÅ‚y blok cenowy
            wrap = await page.query_selector('[data-sentry-element="PriceSection"]')
            if wrap:
                t = (await wrap.inner_text()) or ""
                admin = _extract_admin_from_text(t, rent)
                if admin:
                    admin_src = "header"
    except:
        pass

    # --- 3) czynsz adm. z opisu (jeÅ›li brak w headerze) ---
    if not admin and desc_text:
        admin = _extract_admin_from_text(desc_text, rent)
        if admin:
            admin_src = "opis"

    return rent, admin, admin_src


async def scrape_offer(page: Page, url: str, seen_titles: Set[str], blacklist: Set[str]):
    await page.goto(url, wait_until="domcontentloaded", timeout=45000)
    
    # sprawdÅº twarde markery CF (to rzuci tylko przy â€pewnym" banie)
    try:
        await _detect_cloudflare_block(page)
    except (CloudflareBlocked, CloudfrontBlocked):
        raise  # to jest realny ban
    
    # sprÃ³buj zÅ‚apaÄ‡ selektory, ale timeout â‰  ban
    await asyncio.sleep(random.uniform(0.2, 0.4))  # maÅ‚e opÃ³Åºnienie by DOM siÄ™ uspokoiÅ‚
    try:
        await page.wait_for_selector('[data-cy="adPageHeaderPrice"]', timeout=8000)
    except:
        try:
            await page.wait_for_selector('h1[data-cy="adPageAdTitle"]', timeout=6000)
        except:
            # miÄ™kki retry: odÅ›wieÅ¼ i daj krÃ³tkÄ… szansÄ™ DOM-owi
            try:
                await page.reload(wait_until="domcontentloaded", timeout=15000)
                await asyncio.sleep(random.uniform(0.2, 0.4))  # opÃ³Åºnienie po reload
                await page.wait_for_selector('h1[data-cy="adPageAdTitle"]', timeout=5000)
            except:
                # traktuj jako nieudane ogÅ‚oszenie, ale nie ban
                print(f"[WARN] Timeout selektorÃ³w na {url} â€“ pomijam ofertÄ™ (brak bana).")
                return None
    
    # ponowna szybka kontrola markerÃ³w CF po tym jak DOM siÄ™ narysowaÅ‚
    try:
        await _detect_cloudflare_block(page)
    except (CloudflareBlocked, CloudfrontBlocked):
        raise

    # TYTUÅ (Å¼eby mÃ³c z niego Å‚apaÄ‡)
    title = None
    try:
        el = await page.query_selector('h1[data-cy="adPageAdTitle"]')
        if el: title = (await el.inner_text()).strip()
    except: pass

    # â¬‡â¬‡â¬‡ SKIP jeÅ›li tytuÅ‚ juÅ¼ byÅ‚ (po normalizacji)
    if title and _norm_title(title) in seen_titles:
        oid = extract_id(url)
        print(f"\n[{oid}] SKIP: tytuÅ‚ juÅ¼ przetworzony â†’ '{title}'")
        return None
    
    # â¬‡â¬‡â¬‡ SKIP jeÅ›li tytuÅ‚ jest na blacklist
    if title and _norm_title(title) in blacklist:
        oid = extract_id(url)
        print(f"\n[{oid}] SKIP: tytuÅ‚ na blacklist â†’ '{title}'")
        return None

    # LOKALIZACJE Å¹RÃ“DÅOWE
    header_loc, breadcrumbs = await get_header_location(page)
    desc = await get_description_text(page)

    # CENY: najem + czynsz adm.
    rent_pln = None
    admin_pln = None
    try:
        rent_pln, admin_pln, admin_src = await extract_prices(page, desc)
    except Exception as _e:
        admin_src = ""

    # METRAÅ» m2
    metraz_m2 = None
    metraz_src = ""
    try:
        metraz_m2, metraz_src = await extract_area_m2(page, title, desc)
    except Exception:
        pass

    # ğŸ‘‰ zamiast extract_street_from_sources(...)
    adres = extract_address_for_geocode(header_loc, title, desc)

    oid = extract_id(url)
    print(f"\n[{oid}]:")
    print(f"- Lokalizacja na header: {header_loc or 'â€”'}")
    print(f"- Lokalizacja w opis: {(desc[:180] + ' ...') if desc else 'â€”'}")
    print(f"- Najem (header): {rent_pln if rent_pln is not None else 'â€”'} PLN")
    print(f"- Czynsz adm.: {admin_pln if admin_pln is not None else 'â€”'} PLN"
          + (f" (ÅºrÃ³dÅ‚o: {admin_src})" if admin_pln else ""))
    print(f"- MetraÅ¼: {metraz_m2 if metraz_m2 is not None else 'â€”'} mÂ²"
          + (f" (ÅºrÃ³dÅ‚o: {metraz_src})" if metraz_src else ""))

    if not adres:
        print(f"âŒ NIE UDAÅO ZNALEÅ¹Ä† ULICY DLA: {oid} | {url}")
        return None
    else:
        print(f"- ADRES do geokodera: {adres}")
        # WyczyÅ›Ä‡ adres dla CSV (usuÅ„ prefiksy i ", KrakÃ³w")
        clean_address = _remove_prefix_for_csv(adres)
        print(f"- ADRES do CSV: {clean_address}")
        return {
            "id": oid,
            "title": title,
            "ulica": clean_address,
            "metraz_m2": metraz_m2,   # â¬…ï¸ NOWE
            "url": url,
            "najem_pln": rent_pln,
            "czynsz_adm_pln": admin_pln
        }


from pathlib import Path

def save_to_csv(rows: List[dict], filename: str = "../data/otodom_results.csv"):
    """
    Dopisuje wiersze do CSV. NagÅ‚Ã³wek zapisywany tylko gdy plik nie istnieje lub jest pusty.
    """
    if not rows:
        print("âŒ Brak danych do zapisania (batch pusty)")
        return

    path = Path(filename)
    path.parent.mkdir(parents=True, exist_ok=True)

    write_header = not path.exists() or path.stat().st_size == 0
    fieldnames = ["id", "title", "ulica", "metraz_m2", "najem_pln", "czynsz_adm_pln", "url"]

    with path.open('a', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerows(rows)

    print(f"ğŸ’¾ Dopisano {len(rows)} ogÅ‚oszeÅ„ do {filename}")

def save_partial(rows: List[dict], filename: str = "../data/otodom_results.csv"):
    """
    Alias na save_to_csv â€“ tu przekazujemy juÅ¼ tylko batch (nowe rekordy),
    wiÄ™c nie grozi duplikat.
    """
    save_to_csv(rows, filename)

def save_emergency_backup(rows: List[dict], filename: str = "../data/otodom_emergency_backup.csv"):
    """
    Zapisuje dane do pliku backup w przypadku bÅ‚Ä™du/blokady.
    """
    if not rows:
        print("âŒ Brak danych do zapisania w backup")
        return
    
    path = Path(filename)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Zawsze zapisuj nagÅ‚Ã³wek dla backup
    fieldnames = ["id", "title", "ulica", "metraz_m2", "najem_pln", "czynsz_adm_pln", "url"]
    
    with path.open('w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    
    print(f"ğŸš¨ Zapisano {len(rows)} rekordÃ³w do backup: {filename}")

def add_to_blacklist(title: str, url: str, filename: str = "../data/blacklist.txt"):
    """
    Dodaje ofertÄ™ do blacklist (funkcja do uÅ¼ytku rÄ™cznego).
    """
    if not title:
        print("âŒ Brak tytuÅ‚u do dodania do blacklist")
        return
    
    title_norm = _norm_title(title)
    if not title_norm:
        print("âŒ Nie moÅ¼na znormalizowaÄ‡ tytuÅ‚u")
        return
    
    path = Path(filename)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    with path.open('a', encoding='utf-8') as f:
        f.write(f"{title_norm}\t{url}\n")
    
    print(f"ğŸš« Dodano do blacklist: '{title_norm}'")


# --- RÃ³wnolegÅ‚e scrapowanie ---
CONCURRENCY = 2  # ile ogÅ‚oszeÅ„ naraz - zmniejszone dla stabilnoÅ›ci

# --- Backup w przypadku bana ---



async def scrape_all(context, links, seen_titles: Set[str], progress: Dict[str, object], reserved_titles: Set[str], blacklist: Set[str]):
    """
    Scrapuje wszystkie ogÅ‚oszenia rÃ³wnolegle z limitem wspÃ³Å‚bieÅ¼noÅ›ci.
    progress: {'done': int, 'target': int, 'lock': asyncio.Lock, 'blocked': bool}
    Zwraca (results_list, blocked_bool, collected_count)
    """
    sem = asyncio.Semaphore(CONCURRENCY)
    total_links = len(links)
    results = []
    blocked = False

    async def sem_worker(link, index):
        nonlocal blocked
        async with sem:  # â¬…â¬…â¬… TERAZ DZIAÅA OGRANICZENIE RÃ“WNOLEGÅOÅšCI
            offer_page = await context.new_page()
            try:
                res = await scrape_offer(offer_page, link, seen_titles, blacklist)
                # â¬‡â¬‡â¬‡ dopisz tytuÅ‚ do listy, tylko gdy oferta wejdzie do CSV (res != None)
                if res and res.get("title"):
                    title_norm = _norm_title(res["title"])
                    await append_seen_title(TITLES_FILE, title_norm, TITLES_LOCK, seen_titles)
                    reserved_titles.add(title_norm)  # dodaj do rezerwacji w tym runie
                return res
            except CloudflareBlocked as e:
                print(f"ğŸš¨ Cloudflare BAN: {e}")
                blocked = True
                return None
            except CloudfrontBlocked as e:
                print(f"ğŸš¨ CloudFront BAN: {e}")
                blocked = True
                return None
            except Exception as e:
                # timeouty/losowe bÅ‚Ä™dy nie przerywajÄ… caÅ‚ego runu
                print(f"[WARN] BÅ‚Ä…d przy {link}: {e}")
                return None
            finally:
                await offer_page.close()
                # progress po zamkniÄ™ciu strony (Å¼eby zawsze siÄ™ liczyÅ‚)
                async with progress['lock']:  # type: ignore
                    progress['done'] = int(progress.get('done', 0)) + 1  # type: ignore
                    done = progress['done']  # type: ignore
                    target = progress['target']  # type: ignore
                offer_id = extract_id(link)
                left = max(0, target - done)
                print(f"[{done}/{target}] [{offer_id}] (zostaÅ‚o: {left})")

    print(f"ğŸš€ Rozpoczynam rÃ³wnolegÅ‚e scrapowanie ({CONCURRENCY} ogÅ‚oszeÅ„ naraz)...")
    print(f"ğŸ“‹ ÅÄ…cznie do przetworzenia na tej stronie: {total_links}")

    tasks = [asyncio.create_task(sem_worker(link, i)) for i, link in enumerate(links)]
    batch = await asyncio.gather(*tasks)
    results = [r for r in batch if r]
    return results, blocked, len(results)  # dodatkowo licznik faktycznie zebranych


async def main():
    all_results: List[dict] = []
    collected = 0   # â¬…ï¸ licznik faktycznie zebranych (res != None)
    current_page = 1
    pages_visited = 0
    total_processed = 0
    last_saved_at = 0

    # globalny progress wzglÄ™dem TARGET_OFFERS
    progress = {'done': 0, 'target': TARGET_OFFERS, 'lock': asyncio.Lock(), 'blocked': False}

    async with async_playwright() as p:
        # --- anti-bot: UA + cookies + maÅ‚e losowe pauzy ---
        print("[anti-bot] ustawiam UA/viewport i wchodzÄ™ na homepageâ€¦")
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/127.0.0.0 Safari/537.36"),
            viewport={"width": 1440, "height": 900},
            locale="pl-PL",
            extra_http_headers={"Accept-Language": "pl-PL,pl;q=0.9,en-US;q=0.8,en;q=0.7"}
        )
        
        # Blokada ciÄ™Å¼kich zasobÃ³w - mniej requestÃ³w = mniej szans na bana
        async def _route_filter(route, request):
            rtype = request.resource_type
            if rtype in {"image", "media", "font"}:  # stylesheet zostaw
                await route.abort()
            else:
                await route.continue_()
        await context.route("**/*", _route_filter)
        page = await context.new_page()
        await page.goto("https://www.otodom.pl/", wait_until="domcontentloaded")
        await accept_cookies(page)
        await asyncio.sleep(random.uniform(0.5, 1.2))
        print("poÅ‚Ä…czenie ze stronÄ…: check")

        # --- lista znanych tytuÅ‚Ã³w (persist) ---
        seen_titles = load_seen_titles(TITLES_FILE)
        print(f"ğŸ§  ZaÅ‚adowano {len(seen_titles)} znanych tytuÅ‚Ã³w z {TITLES_FILE}")
        
        # --- lista blacklist (persist) ---
        blacklist = load_blacklist(BLACKLIST_FILE)
        print(f"ğŸš« ZaÅ‚adowano {len(blacklist)} tytuÅ‚Ã³w z blacklist z {BLACKLIST_FILE}")

        try:
            # --- przechodÅº przez strony aÅ¼ zbierzesz X ofert ---
            while collected < TARGET_OFFERS:
                listing_url = f"{LISTING_BASE}&page={current_page}"
                print(f"\nğŸ“„ PrzechodzÄ™ na stronÄ™ {current_page}...")

                await page.goto(listing_url, wait_until="networkidle", timeout=30000)
                await _detect_cloudflare_block(page)  # â¬…ï¸ detekcja blokady listingu
                await page.wait_for_selector('[data-cy="search.listing.organic"]', timeout=30000)

                # 1) Zbierz wpisy (URL + tytuÅ‚) prosto z listingu
                entries = await collect_listing_entries_fast(page)
                
                # â¬‡â¬‡â¬‡ DODAJ:
                if not entries:
                    print("âŒ Listing zwrÃ³ciÅ‚ 0 ogÅ‚oszeÅ„ â€“ koniec wynikÃ³w.")
                    break
                
                # po udanym wczytaniu listingu i zebraniu entries:
                pages_visited += 1
                
                # 2) UÅ¼yj juÅ¼ zaÅ‚adowanych tytuÅ‚Ã³w + RAM-owa â€rezerwacja" na ten przebieg
                reserved_titles: Set[str] = set()

                # 3) Odetnij duplikaty po TYTULE (dynamiczne ID nas nie interesuje)
                new_entries = []
                for it in entries:
                    t_norm = _norm_title(it["title"])  # uÅ¼yj Twojej funkcji normalizujÄ…cej
                    if not t_norm:
                        # Polityka: kompletnie puste tytuÅ‚y omijamy, Å¼eby nie marnowaÄ‡ requestÃ³w.
                        # (jeÅ›li chcesz je jednak Å‚apaÄ‡, usuÅ„ ten 'continue')
                        continue
                    if t_norm in seen_titles or t_norm in reserved_titles:
                        continue
                    new_entries.append(it)
                    reserved_titles.add(t_norm)  # rezerwacja w tym runie

                print(f"ğŸ“‹ Na stronie {current_page}: {len(entries)} ogÅ‚oszeÅ„ (NOWE po tytule: {len(new_entries)})")

                if not new_entries:
                    # Same duplikaty â€“ od razu przejdÅº dalej, bez klikania w ogÅ‚oszenia
                    current_page += 1
                    await asyncio.sleep(random.uniform(0.8, 1.5))
                    continue

                # 4) Dalej pracujesz juÅ¼ na czystej liÅ›cie linkÃ³w:
                links = [it["url"] for it in new_entries]

                # przytnij, jeÅ›li zbliÅ¼amy siÄ™ do TARGET_OFFERS
                if collected + len(links) > TARGET_OFFERS:
                    links = links[:TARGET_OFFERS - collected]

                page_results, blocked, got = await scrape_all(context, links, seen_titles, progress, reserved_titles, blacklist)
                all_results.extend(page_results)
                collected += got
                total_processed += len(links)

                print(f"âœ… Strona {current_page}: {len(page_results)}/{len(links)} ogÅ‚oszeÅ„ z adresami")
                print(f"ğŸ“Š ÅÄ…cznie zebrano: {collected}/{TARGET_OFFERS} ofert")

                # zapis co 50 rekordÃ³w
                if len(all_results) // SAVE_EVERY > last_saved_at // SAVE_EVERY:
                    batch = all_results[last_saved_at:]
                    print(f"ğŸ“ Zapis batchu: {len(batch)} rekordÃ³w (od {last_saved_at} do {len(all_results)-1})")
                    save_partial(batch)
                    last_saved_at = len(all_results)

                if blocked:
                    print("ğŸ›‘ Wykryto blokadÄ™ Cloudflare â€” zapisujÄ™ dane i koÅ„czÄ™.")
                    batch = all_results[last_saved_at:]
                    if batch:
                        save_partial(batch)
                        print(f"ğŸ’¾ Zapisano {len(batch)} rekordÃ³w przed przerwaniem")
                        print(f"ğŸ“Š ÅÄ…cznie zebrano: {len(all_results)} ofert")
                    else:
                        print("â„¹ï¸ Brak nowych danych do zapisania")
                    last_saved_at = len(all_results)
                    break

                if collected >= TARGET_OFFERS:
                    print(f"ğŸ¯ OsiÄ…gniÄ™to cel: {collected}/{TARGET_OFFERS} ofert")
                    break

                current_page += 1
                await asyncio.sleep(random.uniform(1.0, 2.0))

        except (CloudfrontBlocked, CloudflareBlocked) as e:
            print(f"ğŸ›‘ Wykryto blokadÄ™ Cloudflare (poza pÄ™tlÄ…): {e} â€” zapisujÄ™ dane i koÅ„czÄ™.")
            batch = all_results[last_saved_at:]
            if batch:
                save_partial(batch)
                print(f"ğŸ’¾ Zapisano {len(batch)} rekordÃ³w przed przerwaniem")
                print(f"ğŸ“Š ÅÄ…cznie zebrano: {len(all_results)} ofert")
            else:
                print("â„¹ï¸ Brak nowych danych do zapisania")
            last_saved_at = len(all_results)
        except Exception as e:
            print(f"âš ï¸ Nieoczekiwany bÅ‚Ä…d: {e} â€” zapisujÄ™ czÄ™Å›ciowe wyniki.")
            batch = all_results[last_saved_at:]
            if batch:
                save_partial(batch)
                print(f"ğŸ’¾ Zapisano {len(batch)} rekordÃ³w przed przerwaniem")
                print(f"ğŸ“Š ÅÄ…cznie zebrano: {len(all_results)} ofert")
            else:
                print("â„¹ï¸ Brak nowych danych do zapisania")
            last_saved_at = len(all_results)

        print(f"\nğŸ‰ ZakoÅ„czono scrapowanie!")
        print(f"ğŸ“Š Przetworzono {total_processed} ogÅ‚oszeÅ„ z {pages_visited} stron")
        print(f"âœ… Znaleziono adresy dla {collected} ogÅ‚oszeÅ„")

        # finalny zapis (na wszelki wypadek)
        if all_results and last_saved_at < len(all_results):
            batch = all_results[last_saved_at:]
            save_partial(batch)
            last_saved_at = len(all_results)

        await browser.close()


if __name__ == "__main__":
    # Self-test dla ekstraktora adresÃ³w
    tests = [
        ("ul. Macieja Miechowity, Olsza, KrakÃ³w", None, ""),
        ("Aleja 29 Listopada 100, KrakÃ³w", None, ""),
        ("ul. Na KozÅ‚Ã³wce 15, BieÅ¼anÃ³w-Prokocim", None, ""),
        ("rondo Hipokratesa, Mistrzejowice", None, ""),
        ("pl. Wolnica, Kazimierz", None, ""),
        ("al. Space ma przyjemnoÅ›Ä‡ zaprezentowaÄ‡â€¦", None, ""),   # powinno daÄ‡ None
        ("os. Europejskim, Nowa Huta", None, ""),  # powinno daÄ‡ os. Europejskie
        ("Dwa pokoje lub pokÃ³j do wynajÄ™cia", "NadwiÅ›laÅ„ska 11", "Adres: NadwiÅ›laÅ„ska 11"),  # test z numerem
        ("rynek DÄ™bnicki, DÄ™bniki, KrakÃ³w", None, ""),
        ("al.  29, KrakÃ³w", None, "Nowa kawalerka ... Al. 29 Listopada 98."),
        ("al. 29 Listopada 98, KrakÃ³w", None, ""),
        ("ul. pÅ‚k. pil. Stefana Åaszkiewicza, Rakowice, PrÄ…dnik Czerwony, KrakÃ³w, maÅ‚opolskie", None, ""),
        (None, None, "5 minut pieszo na Rynek GÅ‚Ã³wny, Å›wietna lokalizacja przy Karmelickiej 7."),
    ]
    
    # Testy dla metraÅ¼u
    print("\nğŸ§ª Self-test ekstraktora metraÅ¼u:")
    area_tests = [
        ("Mieszkanie, 41,60 mÂ², KrakÃ³w", "41.60"),
        ("Kawalerka 24m2 - Stare DÄ™bniki", "24.0"),
        ("2 pokoje 65 m2", "65.0"),
        ("Powierzchnia: ok.42", "42.0"),
        ("Pow.: 42,5 mkw", "42.5"),
        ("42 m 2", "42.0"),
        ("Mieszkanie 100mÂ² z ogrodem", "100.0"),
        ("Nie ma metraÅ¼u", None),
        ("Cena 2000 zÅ‚ za m2", None),  # nie powinno zÅ‚apaÄ‡ ceny
    ]
    
    for test_input, expected in area_tests:
        result = _area_from_text(test_input) or _area_from_labeled(test_input)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  {status} '{test_input}' â†’ {result} (oczekiwane: {expected})")
    print("ğŸ§ª Self-test ekstraktora adresÃ³w:")
    for header, title, desc in tests:
        result = extract_address_for_geocode(header, title, desc)
        print(f"  header:'{header}' title:'{title}' desc:'{desc[:30]}...' â†’ '{result}'")
    
    # PrzykÅ‚ad uÅ¼ycia blacklist (odkomentuj jeÅ›li chcesz dodaÄ‡ ofertÄ™ do blacklist)
    # add_to_blacklist("PrzykÅ‚adowy tytuÅ‚ oferty", "https://www.otodom.pl/pl/oferta/przyklad-ID123")
    
    print("\nğŸš€ Uruchamiam scraper...")
    asyncio.run(main())
